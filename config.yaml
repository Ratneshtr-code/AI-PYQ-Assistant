project_name: "AI PYQ Assistant"
description: "AI-based platform for semantic search and trend analysis of previous year exam questions"

paths:
  faiss_index: "./embeddings/faiss_index.bin"
  data_csv: "./data/questions.csv"
  model_cache: "./models"

model:
  name: "sentence-transformers/all-MiniLM-L6-v2"

backend:
  host: "127.0.0.1"
  port: 8000
  retrieval_k: 100    # how many vectors to retrieve from FAISS
  min_score: 0.35     # filter threshold
  default_page_size: 3

llm:
  provider: "gemini"  # LLM provider: gemini, openai, etc.
  
  # ==================== MODEL SELECTION ====================
  # Cost-optimized for production:
  #   - gemini-2.0-flash-lite-001 (Lowest cost, requires billing) ‚úÖ CURRENT
  #   - gemini-2.0-flash-001 (Flash, slightly more expensive)
  #   - gemini-2.5-flash (Newest Flash, similar to 1.5 Flash)
  # Note: Flash-Lite models require billing enabled but are 20-30% cheaper
  model: "gemini-2.0-flash-lite-001"  # Lowest cost model - requires billing enabled
  api_key: ""  # Set via GEMINI_API_KEY environment variable or .env file
  temperature: 0.7  # Creativity level (0.0-1.0)
  max_tokens: 2000  # Maximum response length
  
  # ==================== CACHING CONFIGURATION ====================
  # ‚ö†Ô∏è IMPORTANT: Only ONE cache should be enabled at a time!
  # 
  # TESTING CACHE (Development Only):
  #   - Storage: JSON file (data/testing_cache/llm_responses.json)
  #   - Purpose: Developer convenience during testing
  #   - Scope: Local to developer machine
  #   - Cache Key: Question ID + Option + Type (question-based)
  #   - Use Case: Testing same questions repeatedly without burning tokens
  #   - Management: python manage_testing_cache.py
  #
  # PRODUCTION CACHE (Real Users):
  #   - Storage: SQLite database table (llm_explanations)
  #   - Purpose: Cost optimization for real users
  #   - Scope: Shared across all users
  #   - Cache Key: Question ID + Option + Type (question-based)
  #   - Use Case: Reduce production costs by 80% through shared cache
  #   - Management: Database queries
  #
  # QUICK SWITCH:
  #   üß™ TESTING MODE: testing_cache.enabled: true, production_cache.enabled: false
  #   üöÄ PRODUCTION MODE: testing_cache.enabled: false, production_cache.enabled: true
  testing_cache:
    enabled: true  # Set to false when using production cache
    cache_dir: "./data/testing_cache"  # Directory for testing cache JSON files
  
  production_cache:
    enabled: false  # Set to true for production (disables testing cache automatically)
    # When enabled, automatically disables testing_cache
  
  # ==================== PROMPT DUMPING (For Review) ====================
  # Dumps actual prompts and responses sent to/received from LLM
  # Useful for reviewing what instructions are given to LLM
  # Files saved to: data/prompt_dumps/
  prompt_dump:
    enabled: true  # Set to false to disable prompt dumping
    dump_dir: "./data/prompt_dumps"  # Directory for prompt/response dumps

ui:
  port: 8501
  # Maximum number of exams that can be compared in Cross-Exam Insights
  max_exam_comparison: 3  # Default: 3, can be adjusted based on UI/UX requirements
  # Default view mode for My Notes page: "compact" | "grid" | "list"
  default_notes_view: "grid"  # Default: "grid"

# ==================== PAYMENT GATEWAY CONFIGURATION ====================
# Payment gateway settings for subscription purchases
payment:
  # Payment mode: "test" or "production"
  # - test: Shows "Coming Soon" message, no actual payments processed
  # - production: Real payments via Razorpay (requires API keys in .env)
  mode: "test"  # Change to "production" when ready to accept real payments
  
  # Note: Razorpay API keys are set via .env file:
  #   RAZORPAY_KEY_ID=your_key_id
  #   RAZORPAY_KEY_SECRET=your_key_secret
  #   RAZORPAY_WEBHOOK_SECRET=your_webhook_secret
  # 
  # For test mode: No need to install razorpay package or set API keys
  # For production: Install razorpay: pip install razorpay